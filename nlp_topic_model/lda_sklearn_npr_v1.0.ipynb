{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Topic Modeling - LDA\n",
    "\n",
    "For this project you will be working with a dataset of around 12,000 Articles that have no labeled cateogry, and attempting to find 20 cateogries to assign these articles to. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "#### Task: Explore Data. Import pandas and read in the npr.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('~/data/npr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the Articles dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  —   his prefe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article\n",
       "0  In the Washington of 2016, even when the polic...\n",
       "1    Donald Trump has used Twitter  —   his prefe...\n",
       "2    Donald Trump is unabashedly praising Russian...\n",
       "3  Updated at 2:50 p. m. ET, Russian President Vl...\n",
       "4  From photography, illustration and video, to d..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11992, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape # There are total 11992 articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For years now, some of the best, wildest, most moving or revealing stories we’ve been telling ourselves have come not from books, movies or TV, but from video games. So we’re running an occasional series, Reading The Game, in which we take a look at some of these games from a literary perspective. I played the game through the first time in something like a perfect state of awe and terror. Enraptured is, I think, the word that best describes it. Carried away completely into this ruined, beautiful world and the story of Joel and Ellie in The Last of Us. Normally such a completionist  —   so obsessed with exploring every hide and hollow in these imaginary worlds I throw myself into  —   in this instance I simply rolled with the narrative. Ran when running was proper. Slogged through dark and rain and snow and sunshine. Stood my bloody ground when left with no other options. Joel came to love Ellie, his surrogate daughter, and Ellie came to love Joel, the only father she’d ever known. And I (a father, with a daughter roughly Ellie’s age, with Ellie’s   vocabulary and Ellie’s strange, discordant humor) loved Ellie, too. So when I reached the endgame and was presented with a terrible choice (no spoilers . .. yet) I drew my guns and slaughtered my way to the end credits, alight with fury and sure knowledge that I’d made the only choice I could. Second run: The beats are all the same, the story a known thing. Joel and Ellie fight zombies and soldiers and bandits and madmen. They lose friends and see sunrises and, this time, I play with an awful wisdom. Cassandra’s curse. I know how this story ends and I have made up my mind that, this time, I will make the other choice. The right one (morally, mathematically, humanistically) and so I walk with ghosts the whole way, right up to the end, and then . .. And then I make the exact same choice again. I can’t make the other. It hurts too much. Because that is how good the storytelling is in The Last Of Us. It makes you care so deeply for a   bunch of pixels in the shape of a teenage girl that you will damn the whole world twice just for her. (OK, so now we’re gonna get spoilery. Fair warning.) The Last Of Us is a zombie story. It is incredibly derivative, borrows liberally from a hundred different books and movies, is structurally simplistic,   melodramatic, viscerally violent, and despite all this (or, arguably, because of all this) tells one of the most moving, affecting and satisfying stories you’ll find anywhere. At its heart, it is the story of Joel  —   a broken and   thief and smuggler living 20 years deep into a zombie apocalypse. He and his partner, Tess, are forced into a job that requires them to smuggle a young girl out of the Boston quarantine zone and deliver her to an army of revolutionaries because, of course, this girl is The One  —   the only person ever to be immune to the   that turns infected people into gross, murderous mushroom zombies. That young girl is Ellie. And, unsurprisingly, the job does not exactly go as planned. If this all sounds familiar, that’s fine because it is familiar. The   is a stock frame  —   tested and dependable. It is a road trip story in the same way that Cormac McCarthy’s The Road is, or Mad Max: Fury Road. Go from point A to point B, survive the journey, get there whole. And there’s nothing at all wrong with a simple narrative architecture when it is being used to support complex character arcs, as it is here. The Last Of Us is a simple road trip story underneath, existing in service to the complex and rich redemption story on top. All the stakes and ruination are laid out in the first 10 minutes, in a prologue so powerful that it’ll break your heart even if you don’t have one. Joel loses his daughter on the night the world ends, his little girl dying in his arms, under the gun of a panicked soldier trying to hold back the infected. When Ellie floats into his life two decades later, the jaded gamer in you says, Oh, so here’s where he learns to love again. . .. And you’re right. But then you watch it happen  —   in tiny moments like when Ellie, blowing off caution, walks a rickety plank between two buildings and Joel glances briefly down at the watch he wears, a gift from his daughter that he’s been wearing for 20 years  —   and you participate in it happening (protecting her, defending her, eventually becoming her for an extended chunk of the game in a brilliant bit of perspective switching) and it all just clicks. This is a love story  —   one of the best    narratives ever told. Which is when that ending comes and you are presented with the ultimate parental nightmare scenario: Will you sacrifice the life of your child to save the world? Not a stranger, a friend or even a spouse, but your own daughter (which is what Ellie is now  —   Joel’s daughter, blood or no). Because in Ellie lives the cure to the mushroom zombie plague. But in order to create it, she has to die. I started a third playthrough before writing this piece. I am walking slow, taking my time, listening to Ellie read from her joke book, watching her swarmed by fireflies on the outskirts of Boston and admiring the natural beauty and deep environmental storytelling of the game. Nature has reclaimed most of this abandoned world, giving us an unusual apocalypse run riot with wildflowers. And while I have not made it to the end yet, I know it’s coming. I know the choice I’m going to have to make. And I know exactly what I’m going to do. Jason Sheehan is an   a former restaurant critic and the current food editor of Philadelphia magazine. But when no one is looking, he spends his time writing books about spaceships, aliens, giant robots and ray guns. Tales From the Radiation Age is his latest book.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Article'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we see each of the records represents an Article.\n",
    "- The field \"Article\" contains the entire Article text for the respective article.\n",
    "- There is no labeled column to know what this article is about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "#### Objective \n",
    "- To come up with a \"Document Term Matrix (DTM), which is the input to LDA\"\n",
    "\n",
    "#### Steps\n",
    "- Instantiate CountVectorizer with required hyperparameters.\n",
    "- Fit data to CountVectorizer and generate Document Term Matrix (DTM).\n",
    "\n",
    "#### Task: Use CountVectorizer Vectorization to create a vectorized document term matrix. You may want to explore the max_df and min_df parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate CountVectorizer with required hyperparameters\n",
    "# max_df (0.9) => Pick only Words that shows up in 90% of documents\n",
    "# min_df (2)   => Pick only Words that shows up atleast in 2 documents.\n",
    "# Remove Stopwords\n",
    "# We can use Spacy to tokenize and remove stopwords but Countvectorizer does everything for us.\n",
    "\n",
    "cv = CountVectorizer(max_df=0.9,min_df=2,stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=0.9, max_features=None, min_df=2,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "print(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the CountVectorizer, now let's fit the entire data into it and generate Document Term Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.8 s, sys: 594 ms, total: 11.3 s\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create Document Term Matrix\n",
    "dtm = cv.fit_transform(data['Article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11992x54777 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3033388 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It creates a sparse matrix with Number of Articles (Documents) (404289) and Number of words (Terms) (38669)\n",
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Drichlet Allocation\n",
    "\n",
    "#### Steps\n",
    "- 1) Fit the Model with DTM.\n",
    "- 2) Get the Vocabulary of Words.\n",
    "- 3) Get the topics.\n",
    "- 4) Get the highest probability words per topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Fit the Model with DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_components => We want 7 general topics here.\n",
    "# random_state => Some randome number\n",
    "\n",
    "topic_model = LatentDirichletAllocation(n_components=7,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 4s, sys: 5.42 s, total: 7min 9s\n",
      "Wall time: 4min 10s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=7, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=42, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "topic_model.fit(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Get the Vocabulary of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coelho'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"get_feature_names()\"\" method returns a list containing all the words from \"CountVectorizer\"\n",
    "\n",
    "cv.get_feature_names()[10000] # Get the 10000th word in the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Get the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total Number of Topics\n",
    "len(topic_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 54777)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It returns a Numpy array of 7 topics and 54777 words.\n",
    "topic_model.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.64332806e+00, 2.38014333e+03, 1.42900522e-01, ...,\n",
       "        1.43006821e-01, 1.42902042e-01, 1.42861626e-01],\n",
       "       [2.76191749e+01, 5.36394437e+02, 1.42857148e-01, ...,\n",
       "        1.42861973e-01, 1.42857147e-01, 1.42906875e-01],\n",
       "       [7.22783888e+00, 8.24033986e+02, 1.42857148e-01, ...,\n",
       "        6.14236247e+00, 2.14061364e+00, 1.42923753e-01],\n",
       "       ...,\n",
       "       [3.11488651e+00, 3.50409655e+02, 1.42857147e-01, ...,\n",
       "        1.42859912e-01, 1.42857146e-01, 1.42866614e-01],\n",
       "       [4.61486388e+01, 5.14408600e+01, 3.14281373e+00, ...,\n",
       "        1.43107628e-01, 1.43902481e-01, 2.14271779e+00],\n",
       "       [4.93991422e-01, 4.18841042e+02, 1.42857151e-01, ...,\n",
       "        1.42857146e-01, 1.43760101e-01, 1.42866201e-01]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The array It contains Probability of each words in those Topics\n",
    "topic_model.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Get the highest probability words per topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand how it works for a single topic first. After that we will generalize this for all topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_topic = topic_model.components_[0] # Let's get the first topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.64332806e+00, 2.38014333e+03, 1.42900522e-01, ...,\n",
       "       1.43006821e-01, 1.42902042e-01, 1.42861626e-01])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will print the array of Probability of each of the 54777 words for the First Topic\n",
    "single_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2475, 18302, 35285, ..., 22673, 42561, 42993])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the Array using \"argsort\" by index positions from Least to Greatest value.\n",
    "# First index position word i.e 2475th word in the array has the least probability.\n",
    "# Last index position word i.e 42993rd word in the array has the highest probability.\n",
    "single_topic.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33390, 36310, 21228, 10425, 31464,  8149, 36283, 22673, 42561,\n",
       "       42993])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ARGSORT => Index positions sorted from Least to Highest\n",
    "# Top 10 Values (10 Greatest Values) => Last 10 values of ARGSORT()\n",
    "top_ten_words_index = single_topic.argsort()[-10:]\n",
    "top_ten_words_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33390 : 2454.8251962315935 : new\n",
      "36310 : 2529.319133884274 : percent\n",
      "21228 : 2533.2742253067827 : government\n",
      "10425 : 2626.8155405972557 : company\n",
      "31464 : 2628.992410621358 : million\n",
      "8149 : 2760.197440709356 : care\n",
      "36283 : 3643.8261838842077 : people\n",
      "22673 : 3699.3397941251337 : health\n",
      "42561 : 4608.957060251319 : said\n",
      "42993 : 6247.245510521101 : says\n"
     ]
    }
   ],
   "source": [
    "# Print the Top 10 words for first topic (index : probability \" word\")\n",
    "for index in top_ten_words_index:\n",
    "    print(str(index) + ' : ' + str(single_topic[index]) + ' : ' + cv.get_feature_names()[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generalize the above logic for all the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR TOPIC # 0\n",
      "['companies', 'money', 'year', 'federal', '000', 'new', 'percent', 'government', 'company', 'million', 'care', 'people', 'health', 'said', 'says']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC # 1\n",
      "['military', 'house', 'security', 'russia', 'government', 'npr', 'reports', 'says', 'news', 'people', 'told', 'police', 'president', 'trump', 'said']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC # 2\n",
      "['way', 'world', 'family', 'home', 'day', 'time', 'water', 'city', 'new', 'years', 'food', 'just', 'people', 'like', 'says']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC # 3\n",
      "['time', 'new', 'don', 'years', 'medical', 'disease', 'patients', 'just', 'children', 'study', 'like', 'women', 'health', 'people', 'says']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC # 4\n",
      "['voters', 'vote', 'election', 'party', 'new', 'obama', 'court', 'republican', 'campaign', 'people', 'state', 'president', 'clinton', 'said', 'trump']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC # 5\n",
      "['years', 'going', 've', 'life', 'don', 'new', 'way', 'music', 'really', 'time', 'know', 'think', 'people', 'just', 'like']\n",
      "\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC # 6\n",
      "['student', 'years', 'data', 'science', 'university', 'people', 'time', 'schools', 'just', 'education', 'new', 'like', 'students', 'school', 'says']\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 6.14 s, sys: 46.9 ms, total: 6.19 s\n",
      "Wall time: 6.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for index,topic in enumerate(topic_model.components_):\n",
    "    print(f\"THE TOP 15 WORDS FOR TOPIC # {index}\")\n",
    "    print([cv.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Attach Topic Numbers to each of the articles.\n",
    "\n",
    "Probability of a Document belonging to a Particular Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.8 s, sys: 188 ms, total: 20.9 s\n",
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "topic_probability = topic_model.transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02, 0.68, 0.  , 0.  , 0.3 , 0.  , 0.  ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Array containing the Probabilities(rounded to 2 decimal places) of 1st Article belonging to each of the 7 topics.\n",
    "topic_probability[0].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets get the index position of the highest probability\n",
    "topic_probability[0].argmax(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a New Field called \"Topic\" and attach the Topic Number\n",
    "data['Topic'] = topic_probability.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  —   his prefe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article  Topic\n",
       "0  In the Washington of 2016, even when the polic...      1\n",
       "1    Donald Trump has used Twitter  —   his prefe...      1\n",
       "2    Donald Trump is unabashedly praising Russian...      1\n",
       "3  Updated at 2:50 p. m. ET, Russian President Vl...      1\n",
       "4  From photography, illustration and video, to d...      2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Topic Dictionary\n",
    "topic_dict = {0:'topic_0',1:'topic_1',2:'topic_2',3:'topic_3',4:'topic_4',5:'topic_5',6:'topic_6'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Topic_Label'] = data['Topic'].map(topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Topic_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "      <td>1</td>\n",
       "      <td>topic_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  —   his prefe...</td>\n",
       "      <td>1</td>\n",
       "      <td>topic_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "      <td>1</td>\n",
       "      <td>topic_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "      <td>1</td>\n",
       "      <td>topic_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "      <td>2</td>\n",
       "      <td>topic_2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article  Topic Topic_Label\n",
       "0  In the Washington of 2016, even when the polic...      1     topic_1\n",
       "1    Donald Trump has used Twitter  —   his prefe...      1     topic_1\n",
       "2    Donald Trump is unabashedly praising Russian...      1     topic_1\n",
       "3  Updated at 2:50 p. m. ET, Russian President Vl...      1     topic_1\n",
       "4  From photography, illustration and video, to d...      2     topic_2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
